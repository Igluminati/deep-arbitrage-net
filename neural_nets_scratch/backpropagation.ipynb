{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eec15f6",
   "metadata": {},
   "source": [
    "[[Neural Networks from Scratch]]\n",
    "\n",
    "##### Initialise the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1075a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import micropip\n",
    "\n",
    "await micropip.install(\"numpy\")\n",
    "await micropip.install(\"nnfs\")\n",
    "await micropip.install(\"matplotlib\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "from nnfs.datasets import vertical_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89fef1b",
   "metadata": {},
   "source": [
    "\n",
    "##### Backpropogating the ReLU function for a single neuron to act as if we intend to minimise the output for this single neuron (demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e35ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "x = [1.0, -2.0, 3.0]  # inputs\n",
    "w = [-3.0, -1.0, 2.0]  # weights\n",
    "b = 1.0  # bias\n",
    "\n",
    "z = x[0]*w[0] + x[1]*w[1] + x[2]*w[2] + b\n",
    "y = max(z, 0)\n",
    "\n",
    "# Backward pass\n",
    "dvalue = 1.0\n",
    "\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "\n",
    "# Gradients via chain rule\n",
    "dmul_dx0 = w[0] * drelu_dz * dvalue\n",
    "dmul_dx1 = w[1] * drelu_dz * dvalue\n",
    "dmul_dx2 = w[2] * drelu_dz * dvalue\n",
    "\n",
    "dmul_dw0 = x[0] * drelu_dz * dvalue\n",
    "dmul_dw1 = x[1] * drelu_dz * dvalue\n",
    "dmul_dw2 = x[2] * drelu_dz * dvalue\n",
    "\n",
    "print(dmul_dx0, dmul_dw0, dmul_dx1, dmul_dw1, dmul_dx2, dmul_dw2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6479e13c",
   "metadata": {},
   "source": [
    "\n",
    "In the example above the **chained functions** can be loosely interpreted as:\n",
    "$$\n",
    "ReLU(\\sum [inputs\\times weights]+bias)\n",
    "$$\n",
    "or in the form that matches our code more precisely as:\n",
    "$$\n",
    "ReLU(x_0w_0 + x_1w_1 + x_2w_2 + b)\n",
    "$$\n",
    "or:\n",
    "$$\n",
    "ReLU(sum(mul(x_0, w_0), mul(x_1,w_1), mul(x_2, w_2), b))\n",
    "$$\n",
    "The above equation contains 3 nested functions:\n",
    "1. ReLU\n",
    "2. Sum of weighted inputs and a bias\n",
    "3. Multiplications of the inputs and weights\n",
    "\n",
    "##### Why do we need the partial derivatives of the sum components?\n",
    "We need partial derivatives to compute gradients which show how each individual input or weight affects the total sum before activation. This allows us to measure each parameter's direct influence on the output, so we can precisely calculate the gradients needed for updating the parameters during training.\n",
    "\n",
    "##### What is a partial derivative?\n",
    "Measure of how a mulitvariable function (such as $f(x,y)$) changes as one variable changes, will keeping all other variables constant.\n",
    "\n",
    "##### What is the chain rule?\n",
    "The chain rule calculates the rate of change of a function that is composed of other functions.\n",
    "\n",
    "If:\n",
    "$$y=f(z), z=g(x)$$\n",
    "then:\n",
    "$$\\frac{dy}{dx} = \\frac{dy}{dz} \\times \\frac{dz}{dx}$$\n",
    "It multiplies the rate of change of the outer function by the rate of change of the inner function.\n",
    "\n",
    "\n",
    "##### Object Orientated Implementation of Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b278b84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "\tdef __init__(self, n_inputs, n_neurons):\n",
    "\t\tself.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "\t\tself.biases = np.zeros((1, n_neurons))\n",
    "\t# Forward pass\n",
    "\tdef forward(self, inputs):\n",
    "\t\tself.inputs = inputs\n",
    "\t\tself.output = np.dot(inputs, self.weights) + self.biases\n",
    "\t# Backward pass\n",
    "\tdef backward(self, dvalues):\n",
    "\t\t# Gradients on parameters\n",
    "\t\tself.dweights = np.dot(self.inputs.T, dvalues)\n",
    "\t\tself.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\t\t# Gradient on values\n",
    "\t\tself.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "class Activation_ReLU:\n",
    "\t# Forward pass\n",
    "\tdef forward(self, inputs):\n",
    "\t\tself.inputs = inputs\n",
    "\t\tself.output = np.maximum(0, inputs)\n",
    "\t# Backward pass\n",
    "\tdef backward(self, dvalues):\n",
    "\t\t# We need to modify the original variable,\n",
    "\t\t# so we make a copy of the values first\n",
    "\t\tself.dinputs = dvalues.copy()\n",
    "\n",
    "\t\t# Zero gradient where input values were negative\n",
    "\t\tself.dinputs[self.inputs <= 0] = 0\n",
    "class Activation_Softmax:\n",
    "\tdef forward(self,inputs):\n",
    "\t\texp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "\t\tprobabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\t\tself.output = probabilities\n",
    "\n",
    "class Loss:\n",
    "\tdef calculate(self, output, y):\n",
    "\t\tsample_losses = self.forward(output, y)\n",
    "\t\tdata_loss = np.mean(sample_losses)\n",
    "\t\treturn data_loss\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\tdef forward(self, y_pred, y_true):\n",
    "\t\tsamples = len(y_pred)\n",
    "\t\tepsilon = 1e-7\n",
    "\t\ty_pred_clipped = np.clip(y_pred, epsilon, 1-epsilon)\n",
    "\t\t\n",
    "\t\t# When y_true is a vector of scalar class values. E.g [0,1,1]\n",
    "\t\tif len(y_true.shape) == 1:\n",
    "\t\t\tcorrect_confidences = y_pred_clipped[range(samples), y_true]\n",
    "\t\t\n",
    "\t\t# When y_true is an array of vectors (one-hot coding) E.g [[1,0,0], [0,1,0], [0,0,1]]\n",
    "\t\telif len(y_true.shape) == 2:\n",
    "\t\t\tcorrect_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
    "\t\tnegative_log_likelihoods = -np.log(correct_confidences)\n",
    "\t\treturn negative_log_likelihoods\n",
    "\n",
    "\n",
    "# Recreate and run forward pass, then backward pass\n",
    "\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Forward pass\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "print(activation2.output[:5])\n",
    "\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "print(\"Loss:\", loss)\n",
    "\n",
    "# Backward pass\n",
    "\n",
    "# Gradient of loss w.r.t softmax output\n",
    "# Using y to calculate the gradient of the loss with respect to the outputs of each layer starting with Softmax\n",
    "samples = len(activation2.output)\n",
    "if len(y.shape) == 1:\n",
    "    y_one_hot = np.zeros_like(activation2.output)\n",
    "    y_one_hot[np.arange(samples), y] = 1\n",
    "else:\n",
    "    y_one_hot = y\n",
    "\n",
    "dvalues = (activation2.output - y_one_hot) / samples\n",
    "\n",
    "# Backward through second dense layer\n",
    "dense2.backward(dvalues)\n",
    "\n",
    "# Backward through ReLU activation\n",
    "activation1.backward(dense2.dinputs)\n",
    "\n",
    "# Backward through first dense layer\n",
    "dense1.backward(activation1.dinputs)\n",
    "\n",
    "# Display gradients from backward pass (show first 5 values for clarity)\n",
    "print(\"dweights dense2 (first 5):\", dense2.dweights.flatten()[:5])\n",
    "print(\"dbiases dense2:\", dense2.dbiases)\n",
    "print(\"dweights dense1 (first 5):\", dense1.dweights.flatten()[:5])\n",
    "print(\"dbiases dense1:\", dense1.dbiases)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7570a43d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0d29e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d005b23",
   "metadata": {},
   "source": [
    "##### Flow of data visualising the backpropagation process above\n",
    "[Loss]\n",
    "   ↓ (gradient of loss with respect to softmax inputs: p - y)\n",
    "[Softmax Layer]\n",
    "   ↓\n",
    "[Dense Layer 2]\n",
    "   • Calculate dweights2 = a1^T · (p - y)\n",
    "   • Calculate dbiases2 = sum(p - y)\n",
    "   • Calculate dinputs2 = (p - y) · W2^T\n",
    "   ↓\n",
    "[Activation Layer 1 (ReLU)]\n",
    "   • Backprop dinputs2, zero gradients where input ≤ 0\n",
    "   ↓\n",
    "[Dense Layer 1]\n",
    "   • Calculate dweights1 = X^T · dinputs_relu\n",
    "   • Calculate dbiases1 = sum(dinputs_relu)\n",
    "   • Calculate dinputs1 = dinputs_relu · W1^T\n",
    "   ↓\n",
    "[Input X]\n",
    "##### Why do we need derivatives of the operations in each layer?\n",
    "We need derivatives in each layer to understand how changing inputs or weights changes the result. This helps us know exactly how to adjust each weight to reduce loss and improve the model\n",
    "\n",
    "##### Categorical Cross-Entropy Loss Derivative Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d990fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\t#... rest of code\n",
    "\t# Backward pass\n",
    "\tdef backward(self, dvalues, y_true):\n",
    "\t\t# number of samples\n",
    "\t\tsamples = len(dvalues)\n",
    "\t\t# number of labels in every sample\n",
    "\t\tlabels = len(dvalues[0])\n",
    "\t\t# if labels are sparse, turn them into one-hot vector\n",
    "\t\tif len(y_true.shape) == 1:\n",
    "\t\t\ty_true = np.eye(labels)[y_true]\n",
    "\t\t# Calculate gradient\n",
    "\t\tself.dinputs = -y_true / dvalues\n",
    "\t\t# Normalise gradient\n",
    "\t\tself.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d989eae",
   "metadata": {},
   "source": [
    "###### How can you convert a list of class labels like [2, 0, 1] into one-hot encoded vectors using NumPy?\n",
    "Use `np.eye(n)[labels]`, where `n` is the number of classes and `labels` is your list of class indices. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8804dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "labels = [2, 0, 1]\n",
    "one_hot = np.eye(3)[labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6435cf6",
   "metadata": {},
   "source": [
    "produces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd7619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[0., 0., 1.],  # class 2\n",
    "       [1., 0., 0.],  # class 0\n",
    "       [0., 1., 0.]]) # class 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e60b9b",
   "metadata": {},
   "source": [
    "\n",
    "**Next Step:** Complete Backpropagation Support for Softmax + Categorical Cross-Entropy\n",
    "\n",
    "We already have the gradient of the loss with respect to the Softmax output. But this gradient has an efficient combined form for numerical stability and speed.\n",
    "\n",
    "##### Softmax Activation and Categorical Cross-Entropy Classes for Reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2727e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "\tdef forward(self,inputs):\n",
    "\t\texp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "\t\tprobabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True) \n",
    "\t\tself.output = probabilities\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\tdef forward(self, y_pred, y_true):\n",
    "\t\tsamples = len(y_pred)\n",
    "\t\tepsilon = 1e-7\n",
    "\t\ty_pred_clipped = np.clip(y_pred, epsilon, 1-epsilon)\n",
    "\t\t# When y_true is a vector of scalar class values. E.g [0,1,1] \n",
    "\t\tif len(y_true.shape) == 1:\n",
    "\t\t\tcorrect_confidences = y_pred_clipped[range(samples), y_true] \n",
    "\t\t# When y_true is an array of vectors (one-hot coding) E.g [[1,0,0], [0,1,0], [0,0,1]]\n",
    "\t\telif len(y_true.shape) == 2:\n",
    "\t\t\tcorrect_confidences = np.sum(y_pred_clipped*y_true, axis=1) \n",
    "\t\t\tnegative_log_likelihoods = -np.log(correct_confidences) \n",
    "\t\treturn negative_log_likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321cbd65",
   "metadata": {},
   "source": [
    "##### Efficiency Trick: Combine Softmax and Categorical Cross-Entropy\n",
    "We create a new class to handle both forward and backward passess together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc4eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "\tdef __init__(self):\n",
    "\t\tself.activation = Activation_Softmax()\n",
    "\t\tself.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "\tdef forward(self, inputs, y_true):\n",
    "\t\t# Softmax activation\n",
    "\t\tself.activation.forward(inputs)\n",
    "\t\t# Store output\n",
    "\t\tself.output = self.activation.output\n",
    "\t\t# Return loss value\n",
    "\t\treturn self.loss.calculate(self.output, y_true)\n",
    "\n",
    "\tdef backward(self, dvalues, y_true):\n",
    "\t\tsamples = len(dvalues)\n",
    "\t\t# If labels are one-hot, turn them into scalar class indices\n",
    "\t\tif len(y_true.shape) == 2:\n",
    "\t\t\ty_true = np.argmax(y_true, axis=1)\n",
    "\t\t# Copy the probabilities and calculate gradient\n",
    "\t\tself.dinputs = dvalues.copy()\n",
    "\t\tself.dinputs[range(samples), y_true] -= 1\n",
    "\t\tself.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02432f6b",
   "metadata": {},
   "source": [
    "##### Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3f7d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Forward pass\n",
    "loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "# Backward pass\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aaa481",
   "metadata": {},
   "source": [
    "\n",
    "##### Why does this work?\n",
    "When you combine Softmax with Categorical Cross-Entropy, the resulting derivative simplifies to:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_i} = p_i - y_i\n",
    "$$\n",
    "This eliminates the need for manually computing Jacobians or second derivatives and it's faster and more stable.\n",
    "\n",
    "##### Final Output: Gradients Flow Completely Back\n",
    "At this point we have:\n",
    "- Gradients of loss with respect to output layer (via softmax loss combo)\n",
    "- Backprop through all hidden layers (ReLU, Dense)\n",
    "- Access to all parameters gradients: `dweights`, `dbiases`, `dinputs` ready for an optimiser step.\n",
    "\n",
    "**Next steps:**\n",
    "- Implement Optimiser (e.g. SGD)\n",
    "- Implement Training Loop\n",
    "\n",
    "\n",
    "##### Bonus: Backward Propagation Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc19181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample model predictions (probabilities)\n",
    "print(\"Softmax Output (first 5):\")\n",
    "print(loss_activation.output[:5])\n",
    "\n",
    "# Sample loss value\n",
    "print(\"Loss:\")\n",
    "print(loss)\n",
    "\n",
    "# Gradient of loss w.r.t. softmax input (first 5 samples)\n",
    "print(\"dInputs (Softmax Loss Combo) (first 5):\")\n",
    "print(loss_activation.dinputs[:5])\n",
    "\n",
    "# Gradients for Dense Layer 2\n",
    "print(\"Dense2 dWeights (first 5):\")\n",
    "print(dense2.dweights.flatten()[:5])\n",
    "print(\"Dense2 dBiases:\")\n",
    "print(dense2.dbiases)\n",
    "\n",
    "# Gradients for Dense Layer 1\n",
    "print(\"Dense1 dWeights (first 5):\")\n",
    "print(dense1.dweights.flatten()[:5])\n",
    "print(\"Dense1 dBiases:\")\n",
    "print(dense1.dbiases)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
