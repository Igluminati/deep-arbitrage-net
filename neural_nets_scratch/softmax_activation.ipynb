{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d28a36",
   "metadata": {},
   "source": [
    "[[Neural Networks from Scratch]]\n",
    "##### Why Softmax?\n",
    "Our end goal is to measure how right or wrong our prediction is with a probability distribution. If we used ReLU and all the values are negative we can only get an output of 0% which is the main disadvantage of ReLU.\n",
    "\n",
    "Softmax exponentiates each input value, ensuring that it can never be negative or zero whilst retaining its **relative significance**. \n",
    "##### Initialise the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36047426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import micropip\n",
    "\n",
    "await micropip.install(\"numpy\")\n",
    "await micropip.install(\"nnfs\")\n",
    "await micropip.install(\"matplotlib\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import math\n",
    "\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9541e5b",
   "metadata": {},
   "source": [
    "##### Exponentiating Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe836bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "# Euler's number\n",
    "# E = 2.71828182846\n",
    "E = math.e\n",
    "\n",
    "'''\n",
    "exp_values = []\n",
    "\n",
    "for output in layer_outputs:\n",
    "\texp_values.append(E**output)\n",
    "'''\n",
    "\n",
    "exp_values = np.exp(layer_outputs)\n",
    "\n",
    "print(exp_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b202ea08",
   "metadata": {},
   "source": [
    "\n",
    "The next step once these values are exponentiated, is to normalise the values.\n",
    "\n",
    "##### How do we normalise?\n",
    "In our case, we normalise the exponentiated values by dividing a single output neuron's value by the sum of all other output neurons in that output layer.\n",
    "$$\n",
    "y = \\frac{u}{\\sum_{i=1}^{n} u_i}\n",
    "$$\n",
    "##### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111b7114",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "norm_base = sum(exp_values)\n",
    "norm_values = []\n",
    "\n",
    "for value in exp_values:\n",
    "\tnorm_values.append(value / norm_base)\n",
    "'''\n",
    "\n",
    "# Dividing the expontentiated values by the sum of all output neurons in the same output layer\n",
    "norm_values = exp_values / np.sum(exp_values)\n",
    "\n",
    "print(norm_values)\n",
    "print(sum(norm_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2544d9",
   "metadata": {},
   "source": [
    "\n",
    "##### Summing up our progress of Softmax so far\n",
    "Input -> Exponentiate -> Normalise -> Output\n",
    "\n",
    "The combination of the exponentiation and normalisation processes is what makes up Softmax, and it is denoted by the formula below:\n",
    "\n",
    "$$\n",
    "S_{i,j} = \\frac{e^{z_{i,j}}}{\\sum_{l=1}^{L} e^{z_{i,j}}}\n",
    "$$\n",
    "\n",
    "##### Exponentiating and Normalising Batches of Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d8f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [[4.8, 1.21, 2.385],\n",
    "\t\t\t\t [8.9, -1.81, 0.2],\n",
    "\t\t\t\t [1.41, 1.051, 0.026]]\n",
    "\n",
    "# Exponentiating\n",
    "exp_values = np.exp(layer_outputs)\n",
    "# Normalising \n",
    "# axis=1 sums each row; keepdims=True keeps dimensions for broadcasting so we don't have to transpose it\n",
    "norm_values = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "print(norm_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4124d7d",
   "metadata": {},
   "source": [
    "\n",
    "##### What is the issue with exponentiation?\n",
    "As the input to the exponential function grows, we may see an explosion of values where massive numbers and overflows occur.\n",
    "\n",
    "##### What is an overflow?\n",
    "Overflow occurs when a computed value becomes too large for the computer to handle, causing errors.\n",
    "\n",
    "##### How to combat this overflow?\n",
    "Subtract the maximum value in the output vectors from every element. This shifts the largest value to 0 and all others below 0, preventing overflow during exponentiation.\n",
    "\n",
    "##### What happens to the Softmax value?\n",
    "Softmax produces values between 0 and 1 that sum to 1.\n",
    "\n",
    "##### What is a vector?\n",
    "A one-dimensional array - just a list of numbers.\n",
    "\n",
    "##### The Softmax Activation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189abdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "\tdef __init__(self, n_inputs, n_neurons):\n",
    "\t\tself.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "\t\tself.biases = np.zeros((1, n_neurons))\n",
    "\tdef forward(self, inputs):\n",
    "\t\tself.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "class Activation_ReLU:\n",
    "\tdef forward(self, inputs):\n",
    "\t\tself.output = np.maximum(0, inputs)\n",
    "\n",
    "class Activation_Softmax:\n",
    "\tdef forward(self,inputs):\n",
    "\t\texp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "\t\tprobabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\t\tself.output = probabilities\n",
    "\n",
    "# 3 classes associated with the three-armed spiral in the dataset. (Each coordinate must fit to one of the 3 arms)\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# There must be 2 inputs as the only 2 features are X and y coordinates for this data\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "print(activation2.output[:5])"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
