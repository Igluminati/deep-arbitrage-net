{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4411f8b",
   "metadata": {},
   "source": [
    "[[Neural Networks from Scratch]]\n",
    "![[Pasted image 20250605194415.png]]\n",
    "##### What does Binary Logistic Regression predict?\n",
    "Binary Logistic Regression fits a sigmoid curve from y=0 to y=1 to find the probability that a datapoint fits into 1 of 2 classifications such as \"benign\" or \"malignant\" is tumour type detection based on tumour size.\n",
    "\n",
    "It can also be used to assess what variables are useful for classifying samples by seeing a variable's effect on the prediction, and if it is significantly different from 0.\n",
    "\n",
    "##### Sigmoid Activation Function\n",
    "In order to create a fitted sigmoid curve and squash the output between 0 and 1 to gauge the probabilities of the binary outcome, we use a sigmoid activation function denoted by the formula below:\n",
    "$$\n",
    "\\sigma (z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "###### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6d4ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Sigmoid:\n",
    "    def forward(self, inputs):\n",
    "        # Save input and calculate/save output of the sigmoid function\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Derivative - calculates from output of the sigmoid function\n",
    "        self.dinputs = dvalues * (self.output * (1 - self.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e3c76a",
   "metadata": {},
   "source": [
    "\n",
    "##### Binary Cross-Entropy Loss\n",
    "We use this loss function to measure the performance of Binary Logistic Regression. It calculates the loss based on the predicted probabilities and the true binary labels.\n",
    "$$\n",
    "L=-\\frac{1}{N}\\sum{N}{i=1}[y_i log(\\hat{y}_i)+(1-{y_i}log(1-\\hat{y}_i))]\n",
    "$$\n",
    "###### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b92de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_BinaryCrossentropy:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Clip data to prevent division by 0\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "        return sample_losses\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        outputs = len(dvalues[0])\n",
    "        # Clip data to prevent division by 0\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues - (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        # Normalise gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba40a9a0",
   "metadata": {},
   "source": [
    "\n",
    "##### Next Step\n",
    "[[Regression]]"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
