{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d280c25",
   "metadata": {},
   "source": [
    "[[Neural Networks from Scratch]]\n",
    "\n",
    "##### What are batches and why do we use them?\n",
    "A batch is a subset of the full training dataset used in a single forward/backward pass in the neural network. The dataset is split into batches to make the training process more computationally efficient and to stabilise gradient updates. \n",
    "\n",
    "A dataset size of 10,000 samples with a batch size of 32 equates to 10,000/32 = 313 batches to process.\n",
    "\n",
    "##### How to we compute the dot product when dealing with multiple input samples?\n",
    "If you try to derive the dot product of inputs and weights where the shapes are, for instance, both `(3,4)`, there will be an error that arises due to incompatible matrix dimensions for the dot product.\n",
    "\n",
    "For `np.dot(A, B)` to work, `A.shape[1]` must equal `B.shape[0]`.\n",
    "\n",
    "Therefore, the shape of the weights must be `(4,3)` if the shape of the inputs is `(3,4)`. In order to fix the shape of B we transpose the matrix B by switching its rows and columns using `.T` on an `np.array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import micropip\n",
    "await micropip.install(\"numpy\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "inputs = [[1.0, 2.0, 3.0, 2.5],\n",
    "\t\t\t[2.0, 5.0, -1.0, 2.0],\n",
    "\t\t\t[-1.5, 2.7, 3.3, -0.8]] # MATRIX OF VECTORS\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "\t\t\t[0.5, -0.91, 0.26, -0.5],\n",
    "\t\t\t[-0.26, -0.27, 0.17, 0.87]] # MATRIX OF VECTORS\n",
    "\n",
    "biases = [2, 3, 0.5] # VECTOR\n",
    "\n",
    "# layer_outputs = the dot product of inputs and transpose of weights + biases\n",
    "layer1_outputs = np.dot(inputs, np.array(weights).T) + biases\n",
    "print(layer1_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e0b69e",
   "metadata": {},
   "source": [
    "##### The second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd71944",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights2 = [[0.1, -0.14, 0.5],\n",
    "\t\t\t[-0.5, 0.12, -0.33],\n",
    "\t\t\t[-0.44, 0.73, -0.13]]\n",
    "\n",
    "biases2 = [-1, 2, -0.5]\n",
    "\n",
    "layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2\n",
    "print(layer2_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438c68df",
   "metadata": {},
   "source": [
    "\n",
    "##### Converting the layers into an object\n",
    "The standard name for the input data is `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2bbffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[1.0, 2.0, 3.0, 2.5],\n",
    "\t[2.0, 5.0, -1.0, 2.0],\n",
    "\t[-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "class Layer_Dense:\n",
    "\tdef __init__(self, n_inputs, n_neurons):\n",
    "\t\tself.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "\t\tself.biases = np.zeros((1, n_neurons))\n",
    "\tdef forward(self, inputs):\n",
    "\t\tself.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "layer1 = Layer_Dense(4,5)\n",
    "layer2 = Layer_Dense(5,2)\n",
    "\n",
    "layer1.forward(X)\n",
    "#print(f\"Layer 1 Output: {layer1.output}\")\n",
    "layer2.forward(layer1.output)\n",
    "print(f\"Layer 2 Output: {layer2.output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28b351a",
   "metadata": {},
   "source": [
    "\n",
    "##### Why are we multiplying the random weights by `0.10`?\n",
    "The random weights are scaled by a factor of 0.10 in order to reduce their initial magnitude and prevents excessively large outputs early in training.\n",
    "##### Why make biases `np.zeros((1, n_neurons))` where there are two sets of brackets?\n",
    "Biases are shaped `(1, n_neurons)` so they can be added to a layer output shaped `(batch_size, n_neurons`. This ensures each neuron's bias is applied to every sample.\n",
    "##### Why must the shape of `self.weights` in `Layer_Dense` be `(n_inputs, n_neurons)` and not the reverse?\n",
    "This is the ensure the correct orientation required for matrix multiplication with the inputs array `X`."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
