{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd26837d",
   "metadata": {},
   "source": [
    "[[Neural Networks from Scratch]]\n",
    "\n",
    "##### What is Optimisation?\n",
    "The process of adjusting weights and biases to minimise a loss function. This is how we train a model.\n",
    "\n",
    "##### What Does It Use?\n",
    "It uses gradients from backpropagation to determine how much the loss increases if a weight increases:\n",
    "- `dweights`\n",
    "- `dbiases`\n",
    "\n",
    "So we subtract a function of this from each parameter.\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\times \\nabla L\n",
    "$$\n",
    "Where:\n",
    "- $\\theta$ = parameter (weight or bias)\n",
    "- $\\eta$ = learning rate\n",
    "- $\\nabla L$ = gradient of loss with respect to parameter\n",
    "\n",
    "##### Why Learning Rate is Crucial\n",
    "- Too large -> overshoots the minima and diverges\n",
    "- Too small -> extremely slow learning\n",
    "- Must be *tunable* (can **decay** over time)\n",
    "\n",
    "##### What is Decay?\n",
    "As the model learns, the learning rate decreases to make smaller, more precise steps. This is called **learning rate decay**\n",
    "\n",
    "Example:\n",
    "$$\n",
    "lr_{current} = \\frac{lr_{initial}}{1+decay \\times epoch}\n",
    "$$\n",
    "In Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545cfc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "if self.decay:\n",
    "    self.current_learning_rate = (\n",
    "        self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f329a661",
   "metadata": {},
   "source": [
    "##### What is an Epoch?\n",
    "A full pass through the dataset.\n",
    "\n",
    "##### What is Momentum?\n",
    "Imagine pushing a ball down a hill. It speeds up even without a push. Momentum allows updates to build velocity and not get stuck in small bumps\n",
    "$$\n",
    "u_t = \\gamma \\times u_{t-1} - \\eta \\times \\nabla L\n",
    "$$\n",
    "$$\n",
    "\\theta = \\theta + u_t\n",
    "$$\n",
    "Where:\n",
    "- $\\gamma$ = momentum coefficient (0.9 typical)\n",
    "- $u_t$ = velocity (accumulated gradient)\n",
    "\n",
    "##### What Is a Training Loop?\n",
    "It repeats:\n",
    "1. Forward pass -> compute predictions\n",
    "2. Loss calculation\n",
    "3. Accuracy calculation\n",
    "4. Backward pass -> compute gradients\n",
    "5. Optimiser step -> update parameters\n",
    "\n",
    "Repeat for many **epochs** (full passes over data).\n",
    "\n",
    "##### Stochastic Gradient Descent \n",
    "The core idea of **SGD** is:\n",
    "\n",
    "To iteratively update model parameters by taking small steps in the direction that reduces errors, using just a few data points at a time.\n",
    "\n",
    "**Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ffc330",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimiser_SGD:\n",
    "\tdef __init__(self, learning_rate=1.0, decay=0.0, momentum=0.0):\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\t\tself.current_learning_rate = learning_rate\n",
    "\t\tself.decay = decay\n",
    "\t\tself.iterations = 0\n",
    "\t\tself.momentum = momentum\n",
    "\n",
    "\tdef pre_update_params(self):\n",
    "\t\tif self.decay:\n",
    "\t\t\tself.current_learning_rate = self.learning_rate * \\\n",
    "\t\t\t\t(1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "\tdef update_params(self, layer):\n",
    "\t\tif self.momentum:\n",
    "\t\t\tif not hasattr(layer, 'weight_momentums'):\n",
    "\t\t\t\tlayer.weight_momentums = np.zeros_like(layer.weights)\n",
    "\t\t\t\tlayer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "\t\t\tweight_updates = self.momentum * layer.weight_momentums - \\\n",
    "\t\t\t\t\t\t\t self.current_learning_rate * layer.dweights\n",
    "\t\t\tlayer.weight_momentums = weight_updates\n",
    "\n",
    "\t\t\tbias_updates = self.momentum * layer.bias_momentums - \\\n",
    "\t\t\t\t\t\t   self.current_learning_rate * layer.dbiases\n",
    "\t\t\tlayer.bias_momentums = bias_updates\n",
    "\t\telse:\n",
    "\t\t\tweight_updates = -self.current_learning_rate * layer.dweights\n",
    "\t\t\tbias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "\t\tlayer.weights += weight_updates\n",
    "\t\tlayer.biases += bias_updates\n",
    "\n",
    "\tdef post_update_params(self):\n",
    "\t\tself.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff8f303",
   "metadata": {},
   "source": [
    "\n",
    "##### Module Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import micropip\n",
    "\n",
    "await micropip.install(\"numpy\")\n",
    "await micropip.install(\"nnfs\")\n",
    "await micropip.install(\"matplotlib\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import math\n",
    "\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fe87fe",
   "metadata": {},
   "source": [
    "\n",
    "##### Training Loops with SGD Optimiser class (If testing this code block specifically **ensure to first run** `Layer_Dense`, `Activation_ReLU`, `Activation_Softmax_Loss_CategoricalCrossentropy` classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e03c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Network architecture\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Optimiser\n",
    "optimiser = Optimiser_SGD(learning_rate=1.0, decay=1e-3, momentum=0.9)\n",
    "\n",
    "# Training\n",
    "for epoch in range(10001):\n",
    "\tdense1.forward(X)\n",
    "\tactivation1.forward(dense1.output)\n",
    "\tdense2.forward(activation1.output)\n",
    "\tloss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "\t# Accuracy\n",
    "\tpredictions = np.argmax(loss_activation.output, axis=1)\n",
    "\tif len(y.shape) == 2:\n",
    "\t\ty = np.argmax(y, axis=1)\n",
    "\taccuracy = np.mean(predictions == y)\n",
    "\n",
    "\tif not epoch % 100:\n",
    "\t\tprint(f'epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}, lr: {optimiser.current_learning_rate}')\n",
    "\n",
    "\t# Backward\n",
    "\tloss_activation.backward(loss_activation.output, y)\n",
    "\tdense2.backward(loss_activation.dinputs)\n",
    "\tactivation1.backward(dense2.dinputs)\n",
    "\tdense1.backward(activation1.dinputs)\n",
    "\n",
    "\t# Update\n",
    "\toptimiser.pre_update_params()\n",
    "\toptimiser.update_params(dense1)\n",
    "\toptimiser.update_params(dense2)\n",
    "\toptimiser.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecef0bea",
   "metadata": {},
   "source": [
    "\n",
    "##### What do we see here?\n",
    "Key:\n",
    "- epoch = full passes through the dataset\n",
    "- accuracy = self explanatory\n",
    "- loss = measure of how wrong the predictions are\n",
    "- learning rate (lr) = step size for weight updates\n",
    "\n",
    "In the training process using **Stochastic Gradient Descent**, there are large updates initially in order to converge faster. Then, updates become smaller through **decay** so that the **SGD** fine-tunes around the minima and avoids overshooting.\n",
    "\n",
    "##### Next Step\n",
    "[[Better Optimisers (RMSProp, Adam)]]"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
