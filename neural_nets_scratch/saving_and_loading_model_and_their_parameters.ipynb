{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a4afdb4",
   "metadata": {},
   "source": [
    "[[Neural Networks from Scratch]]\n",
    "\n",
    "##### Why Might We Need to Retrieve Model Parameters?\n",
    "There exists situations where we need to inspect model parameters to see if we have dead or exploding neurons. To retrieve said parameters, we will iterate over the trainable layer, take their parameters, and put them into a list. The only trainable layer type that we have here is the Dense layer. Let's add a method to the `Layer_Dense` class to retrieve parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7354a3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer\n",
    "class Layer_Dense:\n",
    "\t# rest of code...\n",
    "\tdef get_parameters(self):\n",
    "\t\treturn self.weights, self.biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eb43a2",
   "metadata": {},
   "source": [
    "\n",
    "Within the `Model` class, we'll add a `get_parameters` method, which will iterate over the trainable layers of the model, run their `get_parameters` method, and append returned weights and biases to a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963a4ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class\n",
    "class Model:\n",
    "\t# rest of code...\n",
    "\tdef get_parameters(self):\n",
    "\t\t# initialises an empty list to store parameters\n",
    "\t\tparameters = []\n",
    "\n",
    "\t\t# iterate over trainable layers and get their parameters\n",
    "\t\tfor layer in self.trainable_layer:\n",
    "\t\t\tparameters.append(layer.get_parameters())\n",
    "\n",
    "\t\t# return a list\n",
    "\t\treturn parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e84c368",
   "metadata": {},
   "source": [
    "\n",
    "After training a model, we get the parameters by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddc1314",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model.get_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3517200a",
   "metadata": {},
   "source": [
    "\n",
    "##### Using `get_parameters` Method in a Practical Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4290e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y, X_test, y_test = create_data_mnist('fashion_mnist_images')\n",
    "# Shuffle the training dataset\n",
    "keys = np.array(range(X.shape[0]))\n",
    "np.random.shuffle(keys)\n",
    "X = X[keys]\n",
    "y = y[keys]\n",
    "# Scale and reshape samples\n",
    "X = (X.reshape(X.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
    "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
    "# Instantiate the model\n",
    "model = Model()\n",
    "# Add layers\n",
    "model.add(Layer_Dense(X.shape[1], 128))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(128, 128))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(128, 10))\n",
    "model.add(Activation_Softmax())\n",
    "# Set loss, optimiser, and accuracy objects\n",
    "model.set(\n",
    "    loss=Loss_CategoricalCrossentropy(),\n",
    "    optimiser=Optimiser_Adam(decay=1e-3),\n",
    "    accuracy=Accuracy_Categorical()\n",
    ")\n",
    "# Finalise the model\n",
    "model.finalise()\n",
    "# Train the model\n",
    "model.train(X, y, validation_data=(X_test, y_test), epochs=10, batch_size=128, print_every=100)\n",
    "# Retrieve and print parameters\n",
    "parameters = model.get_parameters()\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15446a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715ee53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9e7781d",
   "metadata": {},
   "source": [
    "\n",
    "The output:\n",
    "[(array([[ 0.03538642, 0.00794717, -0.04143231, ..., 0.04267325,\n",
    "-0.00935107, 0.01872394],\n",
    "[ 0.03289384, 0.00691249, -0.03424096, ..., 0.02362755,\n",
    "-0.00903602, 0.00977725],\n",
    "[ 0.02189022, -0.01362374, -0.01442819, ..., 0.01320345,\n",
    "-0.02083327, 0.02499157],\n",
    "...,\n",
    "[ 0.0146937 , -0.02869027, -0.02198809, ..., 0.01459295,\n",
    "-0.02335824, 0.00935643],\n",
    "[-0.00090149, 0.01082182, -0.06013806, ..., 0.00704454,\n",
    "-0.0039093 , 0.00311571],\n",
    "[ 0.03660082, -0.00809607, -0.02737131, ..., 0.02216582,\n",
    "-0.01710589, 0.01578414]], dtype=float32), array([[-2.24505737e-02,\n",
    "5.40090213e-03, 2.91307438e-02,\n",
    "-1.04323691e-02, -9.52822249e-03, -1.48109728e-02,\n",
    "...,\n",
    "0.04158591, -0.01614098, -0.0134403 , 0.00708392, 0.0284729 ,\n",
    "0.00336277, -0.00085383, 0.00163819]], dtype=float32)),\n",
    "(array([[-0.00196577, -0.00335329, -0.01362851, ..., 0.00397028,\n",
    "0.00027816, 0.00427755],\n",
    "[ 0.04438829, -0.09197803, 0.02897452, ..., -0.11920264,\n",
    "0.03808296, -0.00536136],\n",
    "[ 0.04146343, -0.03637529, 0.04973305, ..., -0.13564698,\n",
    "-0.08259197, -0.02467288],\n",
    "...,\n",
    "[ 0.03495856, 0.03902597, 0.0028984 , ..., -0.10016892,\n",
    "-0.11356542, 0.05866433],\n",
    "[-0.00857899, -0.02612676, -0.01050871, ..., -0.00551328,\n",
    "-0.01432311, -0.00916382],\n",
    "[-0.20444085, -0.01483698, -0.09321352, ..., 0.02114356,\n",
    "-0.0762504 , 0.03600615]], dtype=float32), array([[-0.0103433 ,\n",
    "-0.00158314, 0.02268587, -0.02352985, -0.02144126,\n",
    "-0.00777614, 0.00795028, -0.00622872, 0.06918745, -0.00743477]],\n",
    "dtype=float32))]\n",
    "\n",
    "##### How Do We Set Parameters in a Model?\n",
    "We implement a `set_parameters` method in `Layer_Dense` and `Model` classes.\n",
    "\n",
    "###### `Layer_Dense`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ca4c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    ...\n",
    "    # Set weights and biases in a layer instance\n",
    "    def set_parameters(self, weights, biases):\n",
    "        self.weights = weights\n",
    "        self.biases = biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d830461",
   "metadata": {},
   "source": [
    "\n",
    "###### `Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00536e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class\n",
    "class Model:\n",
    "    ...\n",
    "    # Updates the model with new parameters\n",
    "    def set_parameters(self, parameters):\n",
    "        # Iterate over the parameters and layers\n",
    "        # and update each layer with each set of the parameters\n",
    "        for parameter_set, layer in zip(parameters, self.trainable_layers):\n",
    "            layer.set_parameters(*parameter_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24716245",
   "metadata": {},
   "source": [
    "\n",
    "Explanation of the code in `Model`:\n",
    "\n",
    "- `zip()` takes a list of parameters and a list of layers and returns an iterator containing tuples of 0th elements of both lists, then the 1st elements of both list, and so on.\n",
    "- With `zip()`, we are iterating over parameters and the layer they belong to at the same time.\n",
    "- Our parameters are a tuple of weights and biases so we **unpack them** with a starred expression `*` so that `Layer_Dense` method can take them as seperate parameters.\n",
    "\n",
    "##### Updating `finalise` function in `Model` class so that we only set a list of trainable layers to the loss function if and only if this loss object exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c83e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class\n",
    "class Model:\n",
    "    ...\n",
    "    # Finalise the model\n",
    "    def finalise(self):\n",
    "        ...\n",
    "        # Update loss object with trainable layers\n",
    "        if self.loss is not None:\n",
    "            self.loss.remember_trainable_layers(self.trainable_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43abd01",
   "metadata": {},
   "source": [
    "\n",
    "##### Changing the `Model` class' `set` function to allow us to pass in only given parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d12a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set loss, optimiser, and accuracy\n",
    "def set(self, *, loss=None, optimiser=None, accuracy=None):\n",
    "    if loss is not None:\n",
    "        self.loss = loss\n",
    "    if optimiser is not None:\n",
    "        self.optimiser = optimiser\n",
    "    if accuracy is not None:\n",
    "        self.accuracy = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357b24e6",
   "metadata": {},
   "source": [
    "\n",
    "##### Training Loop which retrieves the model parameters, creates a new model, and set its parameters with those retrieved from the previously-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b77a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y, X_test, y_test = create_data_mnist('fashion_mnist_images')\n",
    "# Shuffle the training dataset\n",
    "keys = np.array(range(X.shape[0]))\n",
    "np.random.shuffle(keys)\n",
    "X = X[keys]\n",
    "y = y[keys]\n",
    "# Scale and reshape samples\n",
    "X = (X.reshape(X.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
    "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
    "# Instantiate the model\n",
    "model = Model()\n",
    "# Add layers\n",
    "model.add(Layer_Dense(X.shape[1], 128))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(128, 128))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(128, 10))\n",
    "model.add(Activation_Softmax())\n",
    "# Set loss, optimiser, and accuracy objects\n",
    "model.set(\n",
    "    loss=Loss_CategoricalCrossentropy(),\n",
    "    optimiser=Optimiser_Adam(decay=1e-4),\n",
    "    accuracy=Accuracy_Categorical()\n",
    ")\n",
    "# Finalise the model\n",
    "model.finalise()\n",
    "# Train the model\n",
    "model.train(X, y, validation_data=(X_test, y_test), epochs=10, batch_size=128, print_every=100)\n",
    "# Retrieve model parameters\n",
    "parameters = model.get_parameters()\n",
    "# New model\n",
    "# Instantiate the model\n",
    "model = Model()\n",
    "# Add layers\n",
    "model.add(Layer_Dense(X.shape[1], 128))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(128, 128))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(128, 10))\n",
    "model.add(Activation_Softmax())\n",
    "# Set loss and accuracy objects\n",
    "# We do not set optimiser object this time - there's no need to do it\n",
    "# as we won't train the model\n",
    "model.set(\n",
    "    loss=Loss_CategoricalCrossentropy(),\n",
    "    accuracy=Accuracy_Categorical()\n",
    ")\n",
    "# Finalise the model\n",
    "model.finalise()\n",
    "# Set model with parameters instead of training it\n",
    "model.set_parameters(parameters)\n",
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162b326f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634d8e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7ed8822",
   "metadata": {},
   "source": [
    "Output:\n",
    "validation, acc: 0.874, loss: 0.354\n",
    "\n",
    "\n",
    "##### How Do We save and Load Model Parameters?\n",
    "We add a `save_parameters` method in the `Model` class using Python's built-in *pickle* module to serialise any Python object.\n",
    "\n",
    "###### Importing *pickle*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df16a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7508d35e",
   "metadata": {},
   "source": [
    "\n",
    "###### Opening a file in the binary-write mode and saving parameters to it using `pickle.dump`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425f48b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class\n",
    "class Model:\n",
    "    ...\n",
    "    # Saves the parameters to a file\n",
    "    def save_parameters(self, path):\n",
    "        # Open a file in the binary-write mode\n",
    "        # and save parameters to it\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.get_parameters(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c729bdd9",
   "metadata": {},
   "source": [
    "\n",
    "###### Saving parameters of a train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38634a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_parameters('fashion_mnist.parms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffafa75",
   "metadata": {},
   "source": [
    "\n",
    "##### How Do We Load Parameters from a File?\n",
    "We open the file in binary-read mode and have `pickle` read from it, deserialising parameters back into a list.\n",
    "\n",
    "###### calling `set_parameters` method that we created earlier to pass loaded parameters into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8a79f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the weights and updates a model instance with them\n",
    "def load_parameters(self, path):\n",
    "    # Open file in the binary-read mode,\n",
    "    # load weights and update trainable layers\n",
    "    with open(path, 'rb') as f:\n",
    "        self.set_parameters(pickle.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca9261b",
   "metadata": {},
   "source": [
    "\n",
    "###### Setting up a model, loading in the parameters file, and testing the model to check if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde15959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y, X_test, y_test = create_data_mnist('fashion_mnist_images')\n",
    "# Shuffle the training dataset\n",
    "keys = np.array(range(X.shape[0]))\n",
    "np.random.shuffle(keys)\n",
    "X = X[keys]\n",
    "y = y[keys]\n",
    "# Scale and reshape samples\n",
    "X = (X.reshape(X.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
    "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
    "# Instantiate the model\n",
    "model = Model()\n",
    "# Add layers\n",
    "model.add(Layer_Dense(X.shape[1], 128))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(128, 128))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(128, 10))\n",
    "model.add(Activation_Softmax())\n",
    "# Set loss and accuracy objects\n",
    "# We do not set optimiser object this time - there's no need to do it\n",
    "# as we won't train the model\n",
    "model.set(\n",
    "    loss=Loss_CategoricalCrossentropy(),\n",
    "    accuracy=Accuracy_Categorical()\n",
    ")\n",
    "# Finalise the model\n",
    "model.finalise()\n",
    "# Set model with parameters instead of training it\n",
    "model.load_parameters('fashion_mnist.parms')\n",
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de4f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdae7f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e32c0dc2",
   "metadata": {},
   "source": [
    "Output:\n",
    "validation, acc: 0.874, loss: 0.354\n",
    "\n",
    "##### Why Save the Entire Model?\n",
    "Earlier, we saved just weights and biases which can, for instance, initialise a model with those weights, trained from similar data, and then train that model to work with your specific data. This is called transfer learning which loads models faster and uses less memory.\n",
    "\n",
    "Saving the entire model will also allow us to load the optimiser's state and model's structure.\n",
    "\n",
    "###### First we import the `copy` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c40e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6781f6fd",
   "metadata": {},
   "source": [
    "###### Then we make a `copy` method in the `Model` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d9b57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the model\n",
    "def save(self, path):\n",
    "    # Make a deep copy of current model instance\n",
    "    model = copy.deepcopy(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635302d0",
   "metadata": {},
   "source": [
    "\n",
    "While `copy` is faster, it only copies the first level of the object's properties. `deepcopy` recursively traverses all objects and creates a full copy.\n",
    "\n",
    "###### Remove accumulated loss and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd4f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset accumulated values in loss and accuracy objects\n",
    "model.loss.new_pass()\n",
    "model.accuracy.new_pass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a03de4",
   "metadata": {},
   "source": [
    "\n",
    "###### Remove any data in the input layer, and reset the gradients, if any exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f3550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data from input layer\n",
    "# and gradients from the loss object\n",
    "model.input_layer.__dict__.pop('output', None)\n",
    "model.loss.__dict__.pop('dinputs', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43565d56",
   "metadata": {},
   "source": [
    "Both `model.input_layer` and `model.loss` are attributes of the `Model` object but also objects themselves. One of the dunder properties (\"dunder\" = double underscores) that exists for all classes is the `__dict__` property which contains names and values for the class object's properties. \n",
    "\n",
    "We can use the `pop` method on these values which removes them from that instance of the class' object. We set the second parameters of `pop` - the default value that is returned if key doesn't exist - to `None` as we don't intend to catch the removed values.\n",
    "\n",
    "###### Next, we iterate over all the layers to remove their properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a1a842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each layer remove inputs, output and dinputs properties\n",
    "for layer in model.layers:\n",
    "    for property in ['inputs', 'output', 'dinputs', 'dweights', 'dbiases']:\n",
    "        layer.__dict__.pop(property, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5e8fad",
   "metadata": {},
   "source": [
    "\n",
    "With these things cleaned up, we can save the model object.\n",
    "###### Opening a file in binary-write mode, and calling `pickle.dump()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf880a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file in the binary-write mode and save the model\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f71a6c",
   "metadata": {},
   "source": [
    "\n",
    "###### The full `save` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca11ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the model\n",
    "def save(self, path):\n",
    "    # Make a deep copy of current model instance\n",
    "    model = copy.deepcopy(self)\n",
    "    # Reset accumulated values in loss and accuracy objects\n",
    "    model.loss.new_pass()\n",
    "    model.accuracy.new_pass()\n",
    "    # Remove data from the input layer\n",
    "    # and gradients from the loss object\n",
    "    model.input_layer.__dict__.pop('output', None)\n",
    "    model.loss.__dict__.pop('dinputs', None)\n",
    "    # For each layer remove inputs, output and dinputs properties\n",
    "    for layer in model.layers:\n",
    "        for property in ['inputs', 'output', 'dinputs', 'dweights', 'dbiases']:\n",
    "            layer.__dict__.pop(property, None)\n",
    "    # Open a file in the binary-write mode and save the model\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27b6745",
   "metadata": {},
   "source": [
    "\n",
    "###### We can train a model, then save it whenever we wish with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3310c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('fashion_mnist.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04709d11",
   "metadata": {},
   "source": [
    "\n",
    "##### How do we load the entire model?\n",
    "We load a modal ideally before a model object even exists.\n",
    "###### Loading a model by calling a method of the `Model` class instead of the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53d1c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model.load('fashion_mnist.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fb3973",
   "metadata": {},
   "source": [
    "\n",
    "###### Using the `@staticmethod` decorator to run on uninitialised objects where the `self` doesn't exist to immediately create a model object without first needing to instantiate a model object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be95841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads and returns a model\n",
    "@staticmethod\n",
    "def load(path):\n",
    "    # Open file in the binary-read mode, load a model\n",
    "    with open(path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    # Return a model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3032f30",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###### Creating the data, then loading a model to see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d5e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y, X_test, y_test = create_data_mnist('fashion_mnist_images')\n",
    "# Shuffle the training dataset\n",
    "keys = np.array(range(X.shape[0]))\n",
    "np.random.shuffle(keys)\n",
    "X = X[keys]\n",
    "y = y[keys]\n",
    "# Scale and reshape samples\n",
    "X = (X.reshape(X.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
    "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
    "# Load the model\n",
    "model = Model.load('fashion_mnist.model')\n",
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7169dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec816e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc2c687e",
   "metadata": {},
   "source": [
    "Output:\n",
    "validation, acc: 0.874, loss: 0.354\n",
    "\n",
    "Saving the full trained model is a common way of saving a model. It saves parameters (weights and biases) and instances of all the model's objects and the data they generated. \n",
    "\n",
    "Examples of data generated:\n",
    "- optimiser state like cache\n",
    "- learning rate decay\n",
    "- full model structure\n",
    ",etc.\n",
    "\n",
    "##### Next Step\n",
    "[[Prediction Inference]]"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
