{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c22c40c6",
   "metadata": {},
   "source": [
    "[[Neural Networks from Scratch]]\n",
    "\n",
    "### L1 and L2 Regularisation\n",
    "\n",
    "##### Why do we need Regularisation?\n",
    "Regularisation penalises large weights to prevent overfitting.\n",
    "##### L1 (Lasso)\n",
    "Adds a penalty proportional to the **absolute value of weights**, driving some weights to exactly zero.\n",
    "$$\n",
    "Loss_{total} = Loss_{data} + \\lambda \\sum |\\theta|\n",
    "$$\n",
    "**Use Case:** Useful when you have a large number of features and you suspect that only a subset of them are important\n",
    "##### L2 (Ridge)\n",
    "Adds a penalty proportional to the **square of the weights**, shrinking weights but keeping all non-zero and distributing weight energy uniformly.\n",
    "$$\n",
    "Loss_{total} = Loss_{data} + \\lambda \\sum \\theta^2\n",
    "$$\n",
    "**Use Case:** Useful when you have a large number of features and most of them contribute to the outcome\n",
    "##### Dropout\n",
    "Dropout randomly turns off neurons during training to prevent over-reliance on any one path.\n",
    "\n",
    "#### Code Implementation\n",
    "##### Layer Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7fbc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "\tdef __init__(self, n_inputs, n_neurons,\n",
    "\t\t\t\t weight_regulariser_l1=0.0, weight_regulariser_l2=0.0,\n",
    "\t\t\t\t bias_regulariser_l1=0.0, bias_regulariser_l2=0.0):\n",
    "\t\tself.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "\t\tself.biases = np.zeros((1, n_neurons))\n",
    "\t\tself.weight_regulariser_l1 = weight_regulariser_l1\n",
    "\t\tself.weight_regulariser_l2 = weight_regulariser_l2\n",
    "\t\tself.bias_regulariser_l1 = bias_regulariser_l1\n",
    "\t\tself.bias_regulariser_l2 = bias_regulariser_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49290fea",
   "metadata": {},
   "source": [
    "##### Loss Class Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb4d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularization_loss(self, layer):\n",
    "\tloss = 0\n",
    "\tif layer.weight_regulariser_l1 > 0:\n",
    "\t\tloss += layer.weight_regulariser_l1 * np.sum(np.abs(layer.weights))\n",
    "\tif layer.weight_regulariser_l2 > 0:\n",
    "\t\tloss += layer.weight_regulariser_l2 * np.sum(layer.weights ** 2)\n",
    "\tif layer.bias_regulariser_l1 > 0:\n",
    "\t\tloss += layer.bias_regulariser_l1 * np.sum(np.abs(layer.biases))\n",
    "\tif layer.bias_regulariser_l2 > 0:\n",
    "\t\tloss += layer.bias_regulariser_l2 * np.sum(layer.biases ** 2)\n",
    "\treturn loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d72e703",
   "metadata": {},
   "source": [
    "##### Backward Pass In Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85506e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, dvalues):\n",
    "\tself.dweights = np.dot(self.inputs.T, dvalues)\n",
    "\tself.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "\t# L1 weights\n",
    "\tif self.weight_regulariser_l1 > 0:\n",
    "\t\tdL1 = np.ones_like(self.weights)\n",
    "\t\tdL1[self.weights < 0] = -1\n",
    "\t\tself.dweights += self.weight_regulariser_l1 * dL1\n",
    "\n",
    "\t# L2 weights\n",
    "\tif self.weight_regulariser_l2 > 0:\n",
    "\t\tself.dweights += 2 * self.weight_regulariser_l2 * self.weights\n",
    "\n",
    "\t# L1 biases\n",
    "\tif self.bias_regulariser_l1 > 0:\n",
    "\t\tdL1 = np.ones_like(self.biases)\n",
    "\t\tdL1[self.biases < 0] = -1\n",
    "\t\tself.dbiases += self.bias_regulariser_l1 * dL1\n",
    "\n",
    "\t# L2 biases\n",
    "\tif self.bias_regulariser_l2 > 0:\n",
    "\t\tself.dbiases += 2 * self.bias_regulariser_l2 * self.biases\n",
    "\n",
    "\tself.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66ce674",
   "metadata": {},
   "source": [
    "##### Training Loop Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dda80d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loss = loss_function.calculate(predictions, y)\n",
    "reg_loss = loss_function.regularisation_loss(dense1) + \\\n",
    "\t\t   loss_function.regularisation_loss(dense2)\n",
    "loss = data_loss + reg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6696ebe9",
   "metadata": {},
   "source": [
    "##### Example Layer Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a9fcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1 = Layer_Dense(2, 64, weight_regulariser_l2=5e-4, bias_regulariser_l2=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1440eb8",
   "metadata": {},
   "source": [
    "\n",
    "##### Key Observations:\n",
    "- L1 is rarely used alone and often combined with L2\n",
    "- Regularisation terms are mostly applied to hidden layers\n",
    "- Regularisation improves generalisation and tames unstable gradients\n",
    "\n",
    "### Dropout\n",
    "##### Why do we need to implement Dropout?\n",
    "Dropout is a regularisation technique that endeavours to prevent over reliance on any given neuron and ultimately reduce overfitting.\n",
    "\n",
    "##### How does Dropout work?\n",
    "During training, Dropout randomly disables a fraction of neurons by zeroing their outputs.\n",
    "![[Pasted image 20250605183714.png]]\n",
    "\n",
    "This is done by generating a Bernoulli mask that acts as a random filter to temporarily deactivate certain neurons:\n",
    "$$\n",
    "mask \\sim Binomial(1,1-dropout_{rate})\n",
    "$$\n",
    "Then:\n",
    "$$\n",
    "output = inputs \\times mask/(1 - dropout_{rate})\n",
    "$$\n",
    "During inference, Dropout is **omitted**.\n",
    "\n",
    "#### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179cea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dropout:\n",
    "\tdef __init__(self, rate):\n",
    "\t\t# Keep rate, not drop rate\n",
    "\t\tself.rate = 1 - rate\n",
    "\n",
    "\tdef forward(self, inputs, training):\n",
    "\t\tself.inputs = inputs\n",
    "\t\tif training:\n",
    "\t\t\t# Sample dropout mask and scale\n",
    "\t\t\tself.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "\t\t\tself.output = inputs * self.binary_mask\n",
    "\t\telse:\n",
    "\t\t\tself.output = inputs\n",
    "\n",
    "\tdef backward(self, dvalues):\n",
    "\t\tself.dinputs = dvalues * self.binary_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbf7ae6",
   "metadata": {},
   "source": [
    "##### Integration into Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f8334",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1 = Layer_Dense(2, 64, weight_regulariser_l2=5e-4, bias_regulariser_l2=5e-4)\n",
    "activation1 = Activation_ReLU()\n",
    "dropout1 = Layer_Dropout(0.1)\n",
    "dense2 = Layer_Dense(64, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff16b1d",
   "metadata": {},
   "source": [
    "##### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7a1d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "dropout1.forward(activation1.output, training=True)\n",
    "dense2.forward(dropout1.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea613dec",
   "metadata": {},
   "source": [
    "##### Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1078c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_activation.backward(dense2.output, y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "dropout1.backward(dense2.dinputs)\n",
    "activation1.backward(dropout1.dinputs)\n",
    "dense1.backward(activation1.dinputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df01a7b",
   "metadata": {},
   "source": [
    "##### Optimiser Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f79d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = Optimiser_Adam(learning_rate=0.05, decay=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1927f263",
   "metadata": {},
   "source": [
    "##### Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebc04f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before dropout\n",
    "np.sum(output)\n",
    "\n",
    "# After dropout\n",
    "np.sum(output * dropout_mask / (1 - dropout_rate)) â‰ˆ original sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c42c804",
   "metadata": {},
   "source": [
    "\n",
    "### Full Code Up To This Point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6415a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    # Layer initialisation\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regulariser_l1=0, weight_regulariser_l2=0,\n",
    "                 bias_regulariser_l1=0, bias_regulariser_l2=0):\n",
    "        # Initialise weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # Set regularisation strength\n",
    "        self.weight_regulariser_l1 = weight_regulariser_l1\n",
    "        self.weight_regulariser_l2 = weight_regulariser_l2\n",
    "        self.bias_regulariser_l1 = bias_regulariser_l1\n",
    "        self.bias_regulariser_l2 = bias_regulariser_l2\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights, and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "        # Gradients on regularisation\n",
    "        # L1 on weights\n",
    "        if self.weight_regulariser_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regulariser_l1 * dL1\n",
    "        # L2 on weights\n",
    "        if self.weight_regulariser_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regulariser_l2 * self.weights\n",
    "        # L1 on biases\n",
    "        if self.bias_regulariser_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regulariser_l1 * dL1\n",
    "        # L2 on biases\n",
    "        if self.bias_regulariser_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regulariser_l2 * self.biases\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "class Layer_Dropout:\n",
    "    # Initialisation\n",
    "    def __init__(self, rate):\n",
    "        # Store rate, we invert it as, for example, for dropout of 0.1, we need a success rate of 0.9\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input values\n",
    "        self.inputs = inputs\n",
    "        # Generate and save scaled mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        # Apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify the original variable, let's make a copy of the values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Get unnormalised probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalise them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialised array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "\n",
    "class Optimiser_SGD:\n",
    "    # Initialise optimiser - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "            # If layer does not contain momentum arrays, create them filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            # Build bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimiser_Adagrad:\n",
    "    # Initialise optimiser - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays, create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalisation with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimiser_RMSprop:\n",
    "    # Initialise optimiser - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays, create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalisation with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimiser_Adam:\n",
    "    # Initialise optimiser - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays, create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "        # Get corrected momentum\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "\n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Vanilla SGD parameter update + normalisation with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Loss:\n",
    "    # Regularisation loss calculation\n",
    "    def regularization_loss(self, layer):\n",
    "        regularization_loss = 0\n",
    "        # L1 regularisation - weights\n",
    "        if layer.weight_regulariser_l1 > 0:\n",
    "            regularisation_loss += layer.weight_regulariser_l1 * np.sum(np.abs(layer.weights))\n",
    "        # L2 regularisation - weights\n",
    "        if layer.weight_regulariser_l2 > 0:\n",
    "            regularisation_loss += layer.weight_regulariser_l2 * np.sum(layer.weights * layer.weights)\n",
    "        # L1 regularisation - biases\n",
    "        if layer.bias_regulariser_l1 > 0:\n",
    "            regularisation_loss += layer.bias_regulariser_l1 * np.sum(np.abs(layer.biases))\n",
    "        # L2 regularisation - biases\n",
    "        if layer.bias_regulariser_l2 > 0:\n",
    "            regularisation_loss += layer.bias_regulariser_l2 * np.sum(layer.biases * layer.biases)\n",
    "        return regularisation_loss\n",
    "\n",
    "    # Calculates the data and regularisation losses given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=1000, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64, weight_regulariser_l2=5e-4, bias_regulariser_l2=5e-4)\n",
    "\n",
    "# Create ReLU activation\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create dropout layer\n",
    "dropout1 = Layer_Dropout(0.1)\n",
    "\n",
    "# Create second Dense layer with 64 input features and 3 output values\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimiser\n",
    "optimiser = Optimiser_Adam(learning_rate=0.05, decay=5e-5)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dropout1.forward(activation1.output)\n",
    "    dense2.forward(dropout1.output)\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate regularisation penalty\n",
    "    regularization_loss = loss_activation.loss.regularization_loss(dense1) + loss_activation.loss.regularization_loss(dense2)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f} (data_loss: {data_loss:.3f}, reg_loss: {regularization_loss:.3f}), lr: {optimiser.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    dropout1.backward(dense2.dinputs)\n",
    "    activation1.backward(dropout1.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimiser.pre_update_params()\n",
    "    optimiser.update_params(dense1)\n",
    "    optimiser.update_params(dense2)\n",
    "    optimiser.post_update_params()\n",
    "\n",
    "# Validate the model\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbb9e6c",
   "metadata": {},
   "source": [
    "\n",
    "##### Results:\n",
    "![[Screenshot_2025-06-05_18-50-46.png]]\n",
    "\n",
    "New components in the results:\n",
    "\n",
    "- Data Loss (`data_loss`): This measures how well the model's predictions match the actual target values. It is the primary component of the loss function\n",
    "- Regularisation Loss (`reg_loss`): This is an additional term added to the loss function to penalise large weights in the model, helping to prevent overfitting\n",
    "\n",
    "Summary:\n",
    "- The model is trained over 10,000 epochs\n",
    "- The model achieves a validation accuracy of 0.753 and a validation loss of 0.692\n",
    "\n",
    "##### Next Step\n",
    "[[Binary Logistic Regression]]"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
