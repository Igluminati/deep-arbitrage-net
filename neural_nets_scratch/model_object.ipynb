{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4db3e4b1",
   "metadata": {},
   "source": [
    "[[Neural Networks from Scratch]]\n",
    "\n",
    "##### What is the point of this chapter?\n",
    "To encapsulate and abstract the neural network training logic into a reusable, modular class structure, removing duplication.\n",
    "\n",
    "##### Full Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32393140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class\n",
    "class Model:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Create a list of network objects\n",
    "        self.layers = []\n",
    "        # Softmax classifier's output object\n",
    "        self.softmax_classifier_output = None\n",
    "\n",
    "    # Add objects to the model\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # Set loss, optimiser and accuracy\n",
    "    def set(self, *, loss, optimiser, accuracy):\n",
    "        self.loss = loss\n",
    "        self.optimiser = optimiser\n",
    "        self.accuracy = accuracy\n",
    "\n",
    "    # Finalise the model\n",
    "    def finalise(self):\n",
    "        # Create and set the input layer\n",
    "        self.input_layer = Layer_Input()\n",
    "\n",
    "        # Count all the objects\n",
    "        layer_count = len(self.layers)\n",
    "\n",
    "        # Initialise a list containing trainable layers:\n",
    "        self.trainable_layers = []\n",
    "\n",
    "        # Iterate the objects\n",
    "        for i in range(layer_count):\n",
    "\n",
    "            # If it's the first layer,\n",
    "            # the previous layer object is the input layer\n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "\n",
    "            # All layers except for the first and the last\n",
    "            elif i < layer_count - 1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "\n",
    "            # The last layer - the next object is the loss\n",
    "            # Also let's save aside the reference to the last object\n",
    "            # whose output is the model's output\n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "\n",
    "            # If layer contains an attribute called \"weights\",\n",
    "            # it's a trainable layer -\n",
    "            # add it to the list of trainable layers\n",
    "            # We don't need to check for biases -\n",
    "            # checking for weights is enough\n",
    "            if hasattr(self.layers[i], 'weights'):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "\n",
    "        # Update loss object with trainable layers\n",
    "        self.loss.remember_trainable_layers(\n",
    "            self.trainable_layers)\n",
    "\n",
    "        # If output activation is Softmax and\n",
    "        # loss function is Categorical Cross-Entropy\n",
    "        # create an object of combined activation\n",
    "        # and loss function containing\n",
    "        # faster gradient calculation\n",
    "        if isinstance(self.layers[-1], Activation_Softmax) and isinstance(self.loss, Loss_CategoricalCrossentropy):\n",
    "            self.softmax_classifier_output = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Performs forward pass\n",
    "    def forward(self, X, training):\n",
    "        # Call forward method on the input layer\n",
    "        self.input_layer.forward(X, training)\n",
    "\n",
    "        # Call forward method of every object in a chain\n",
    "        # Pass output of the previous object as a parameter\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output, training)\n",
    "\n",
    "        # \"layer\" is now the last object from the list,\n",
    "        # return its output\n",
    "        return layer.output\n",
    "\n",
    "    # Performs backward pass\n",
    "    def backward(self, output, y):\n",
    "        # If softmax classifier\n",
    "        if self.softmax_classifier_output is not None:\n",
    "            # First call backward method\n",
    "            # on the combined activation/loss\n",
    "            # this will set dinputs property\n",
    "            self.softmax_classifier_output.backward(output, y)\n",
    "\n",
    "            # Since we'll not call backward method of the last layer\n",
    "            # which is Softmax activation\n",
    "            # as we used combined activation/loss\n",
    "            # object, let's set dinputs in this object\n",
    "            self.layers[-1].dinputs = \\\n",
    "                self.softmax_classifier_output.dinputs\n",
    "\n",
    "            # Call backward method going through\n",
    "            # all the objects but last\n",
    "            # in reversed order passing dinputs as a parameter\n",
    "            for layer in reversed(self.layers[:-1]):\n",
    "                layer.backward(layer.next.dinputs)\n",
    "\n",
    "            return\n",
    "\n",
    "        # First call backward method on the loss\n",
    "        # this will set dinputs property that the last\n",
    "        # layer will try to access shortly\n",
    "        self.loss.backward(output, y)\n",
    "\n",
    "        # Call backward method going through all the objects\n",
    "        # in reversed order passing dinputs as a parameter\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "\n",
    "    # Train the model\n",
    "    def train(self, X, y, *, epochs=1, print_every=1,\n",
    "              validation_data=None):\n",
    "        # Initialise accuracy object\n",
    "        self.accuracy.init(y)\n",
    "\n",
    "        # Main training loop\n",
    "        for epoch in range(1, epochs+1):\n",
    "\n",
    "            # Perform the forward pass\n",
    "            output = self.forward(X, training=True)\n",
    "\n",
    "            # Calculate loss\n",
    "            data_loss, regularisation_loss = \\\n",
    "                self.loss.calculate(output, y,\n",
    "                                    include_regularisation=True)\n",
    "            loss = data_loss + regularisation_loss\n",
    "\n",
    "            # Get predictions and calculate an accuracy\n",
    "            predictions = self.output_layer_activation.predictions(\n",
    "                              output)\n",
    "            accuracy = self.accuracy.calculate(predictions, y)\n",
    "\n",
    "            # Perform backward pass\n",
    "            self.backward(output, y)\n",
    "\n",
    "            # Optimise (update parameters)\n",
    "            self.optimiser.pre_update_params()\n",
    "            for layer in self.trainable_layers:\n",
    "                self.optimiser.update_params(layer)\n",
    "            self.optimiser.post_update_params()\n",
    "\n",
    "            # Print a summary\n",
    "            if not epoch % print_every:\n",
    "                print(f'epoch: {epoch}, '\n",
    "                      f'acc: {accuracy:.3f}, '\n",
    "                      f'loss: {loss:.3f} ('\n",
    "                      f'data_loss: {data_loss:.3f}, '\n",
    "                      f'reg_loss: {regularisation_loss:.3f}), '\n",
    "                      f'lr: {self.optimiser.current_learning_rate}')\n",
    "\n",
    "        # If there is the validation data\n",
    "        if validation_data is not None:\n",
    "\n",
    "            # For better readability\n",
    "            X_val, y_val = validation_data\n",
    "\n",
    "            # Perform the forward pass\n",
    "            output = self.forward(X_val, training=False)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = self.loss.calculate(output, y_val)\n",
    "\n",
    "            # Get predictions and calculate an accuracy\n",
    "            predictions = self.output_layer_activation.predictions(\n",
    "                              output)\n",
    "            accuracy = self.accuracy.calculate(predictions, y_val)\n",
    "\n",
    "            # Print a summary\n",
    "            print(f'validation, '\n",
    "                  f'acc: {accuracy:.3f}, '\n",
    "                  f'loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f98864",
   "metadata": {},
   "source": [
    "\n",
    "##### Full Model Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b7064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sine_data()\n",
    "\n",
    "model = Model()\n",
    "model.add(Layer_Dense(1, 64))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(64, 64))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(64, 1))\n",
    "model.add(Activation_Linear())\n",
    "\n",
    "model.set(\n",
    "    loss=Loss_MeanSquaredError(),\n",
    "    optimiser=Optimiser_Adam(learning_rate=0.005, decay=1e-3),\n",
    "    accuracy=Accuracy_Regression()\n",
    ")\n",
    "\n",
    "model.finalise()\n",
    "model.train(X, y, epochs=10000, print_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e49cc14",
   "metadata": {},
   "source": [
    "\n",
    "##### Next Step:\n",
    "[[A Real Dataset]]"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
