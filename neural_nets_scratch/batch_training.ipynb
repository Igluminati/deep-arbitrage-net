{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0336ac4b",
   "metadata": {},
   "source": [
    "[[Neural Networks from Scratch]]\n",
    "\n",
    "##### Why Batch Training?\n",
    "Instead of using the whole dataset in a single go, we split it into small fixed-size batches and train on each one sequentially.\n",
    "\n",
    "Why?\n",
    "- Fits into memory\n",
    "- Reduces gradient noise\n",
    "- Faster convergence to minimum loss\n",
    "\n",
    "##### Accuracy Measurement (Classification)\n",
    "We measure how many predictions match the true labels. In classification:\n",
    "$$\n",
    "accuracy = \\frac{correct \\space predictions}{total \\space predictions}\n",
    "$$\n",
    "For softmax outputs, we:\n",
    "- Take `argmax` over model outputs -> predicted class\n",
    "- Compare to `y_true` (actual class indices)\n",
    "\n",
    "##### Full Integration Example: Spiral Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8952f7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\tdef __init__(self, n_inputs, n_neurons):\n",
    "\t\tself.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "\t\tself.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "\tdef forward(self, inputs):\n",
    "\t\tself.inputs = inputs\n",
    "\t\tself.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\tdef backward(self, dvalues):\n",
    "\t\tself.dweights = np.dot(self.inputs.T, dvalues)\n",
    "\t\tself.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\t\tself.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "# ReLU\n",
    "class Activation_ReLU:\n",
    "\tdef forward(self, inputs):\n",
    "\t\tself.inputs = inputs\n",
    "\t\tself.output = np.maximum(0, inputs)\n",
    "\n",
    "\tdef backward(self, dvalues):\n",
    "\t\tself.dinputs = dvalues.copy()\n",
    "\t\tself.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "# Softmax\n",
    "class Activation_Softmax:\n",
    "\tdef forward(self, inputs):\n",
    "\t\texp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "\t\tprobabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\t\tself.output = probabilities\n",
    "\n",
    "# Combined softmax + cross-entropy loss\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "\tdef __init__(self):\n",
    "\t\tself.activation = Activation_Softmax()\n",
    "\t\tself.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "\tdef forward(self, inputs, y_true):\n",
    "\t\tself.activation.forward(inputs)\n",
    "\t\tself.output = self.activation.output\n",
    "\t\treturn self.loss.calculate(self.output, y_true)\n",
    "\n",
    "\tdef backward(self, dvalues, y_true):\n",
    "\t\tsamples = len(dvalues)\n",
    "\t\tif len(y_true.shape) == 2:\n",
    "\t\t\ty_true = np.argmax(y_true, axis=1)\n",
    "\t\tself.dinputs = dvalues.copy()\n",
    "\t\tself.dinputs[range(samples), y_true] -= 1\n",
    "\t\tself.dinputs = self.dinputs / samples\n",
    "\n",
    "# Loss\n",
    "class Loss:\n",
    "\tdef calculate(self, output, y):\n",
    "\t\tsample_losses = self.forward(output, y)\n",
    "\t\tdata_loss = np.mean(sample_losses)\n",
    "\t\treturn data_loss\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\tdef forward(self, y_pred, y_true):\n",
    "\t\tsamples = len(y_pred)\n",
    "\t\tepsilon = 1e-7\n",
    "\t\ty_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "\n",
    "\t\tif len(y_true.shape) == 1:\n",
    "\t\t\tcorrect_confidences = y_pred_clipped[range(samples), y_true]\n",
    "\t\telif len(y_true.shape) == 2:\n",
    "\t\t\tcorrect_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "\t\tnegative_log_likelihoods = -np.log(correct_confidences)\n",
    "\t\treturn negative_log_likelihoods\n",
    "\n",
    "# Optimiser\n",
    "class Optimiser_Adam:\n",
    "\tdef __init__(self, learning_rate=0.02, decay=5e-5, epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\t\tself.current_learning_rate = learning_rate\n",
    "\t\tself.decay = decay\n",
    "\t\tself.iterations = 0\n",
    "\t\tself.epsilon = epsilon\n",
    "\t\tself.beta_1 = beta_1\n",
    "\t\tself.beta_2 = beta_2\n",
    "\n",
    "\tdef pre_update_params(self):\n",
    "\t\tif self.decay:\n",
    "\t\t\tself.current_learning_rate = self.learning_rate * \\\n",
    "\t\t\t\t(1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "\tdef update_params(self, layer):\n",
    "\t\tif not hasattr(layer, 'weight_cache'):\n",
    "\t\t\tlayer.weight_momentums = np.zeros_like(layer.weights)\n",
    "\t\t\tlayer.weight_cache = np.zeros_like(layer.weights)\n",
    "\t\t\tlayer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\t\t\tlayer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "\t\tlayer.weight_momentums = self.beta_1 * layer.weight_momentums + \\\n",
    "\t\t\t\t\t\t\t\t (1 - self.beta_1) * layer.dweights\n",
    "\t\tlayer.bias_momentums = self.beta_1 * layer.bias_momentums + \\\n",
    "\t\t\t\t\t\t\t\t(1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "\t\tcorrected_weight_momentums = layer.weight_momentums / \\\n",
    "\t\t\t\t\t\t\t\t\t (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\t\tcorrected_bias_momentums = layer.bias_momentums / \\\n",
    "\t\t\t\t\t\t\t\t   (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "\t\tlayer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "\t\t\t\t\t\t\t (1 - self.beta_2) * layer.dweights**2\n",
    "\t\tlayer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "\t\t\t\t\t\t   (1 - self.beta_2) * layer.dbiases**2\n",
    "\n",
    "\t\tcorrected_weight_cache = layer.weight_cache / \\\n",
    "\t\t\t\t\t\t\t\t (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\t\tcorrected_bias_cache = layer.bias_cache / \\\n",
    "\t\t\t\t\t\t\t   (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "\t\tlayer.weights += -self.current_learning_rate * corrected_weight_momentums / \\\n",
    "\t\t\t\t\t\t (np.sqrt(corrected_weight_cache) + self.epsilon)\n",
    "\t\tlayer.biases += -self.current_learning_rate * corrected_bias_momentums / \\\n",
    "\t\t\t\t\t\t(np.sqrt(corrected_bias_cache) + self.epsilon)\n",
    "\n",
    "\tdef post_update_params(self):\n",
    "\t\tself.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0343b51d",
   "metadata": {},
   "source": [
    "\n",
    "##### Training Loop with Batching and Accuracy (If testing this code block specifically **ensure to first run**  the full integrated example above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf56a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X, y = spiral_data(samples=1000, classes=3)\n",
    "\n",
    "# Model\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimiser = Optimiser_Adam(learning_rate=0.05, decay=5e-7)\n",
    "\n",
    "# Batch settings\n",
    "batch_size = 128\n",
    "steps = X.shape[0] // batch_size\n",
    "if X.shape[0] % batch_size != 0:\n",
    "\tsteps += 1\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10001):\n",
    "\tepoch_loss = 0\n",
    "\tepoch_accuracy = 0\n",
    "\n",
    "\tfor step in range(steps):\n",
    "\t\tbatch_X = X[step * batch_size:(step + 1) * batch_size]\n",
    "\t\tbatch_y = y[step * batch_size:(step + 1) * batch_size]\n",
    "\n",
    "\t\tdense1.forward(batch_X)\n",
    "\t\tactivation1.forward(dense1.output)\n",
    "\t\tdense2.forward(activation1.output)\n",
    "\n",
    "\t\tloss = loss_activation.forward(dense2.output, batch_y)\n",
    "\t\tepoch_loss += loss\n",
    "\n",
    "\t\tpredictions = np.argmax(loss_activation.output, axis=1)\n",
    "\t\taccuracy = np.mean(predictions == batch_y)\n",
    "\t\tepoch_accuracy += accuracy\n",
    "\n",
    "\t\tloss_activation.backward(loss_activation.output, batch_y)\n",
    "\t\tdense2.backward(loss_activation.dinputs)\n",
    "\t\tactivation1.backward(dense2.dinputs)\n",
    "\t\tdense1.backward(activation1.dinputs)\n",
    "\n",
    "\t\toptimiser.pre_update_params()\n",
    "\t\toptimiser.update_params(dense1)\n",
    "\t\toptimiser.update_params(dense2)\n",
    "\t\toptimiser.post_update_params()\n",
    "\n",
    "\t# Report\n",
    "\tif epoch % 100 == 0:\n",
    "\t\tprint(f'epoch: {epoch}, acc: {epoch_accuracy / steps:.3f}, loss: {epoch_loss / steps:.3f}, lr: {optimiser.current_learning_rate}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f792f",
   "metadata": {},
   "source": [
    "##### What do we see here?\n",
    "- Efficient **batch training**\n",
    "- Stable **accuracy tracking**\n",
    "- Full integrated **adaptive optimiser (adam)**\n",
    "##### Next Step\n",
    "[[L1 and L2 Regularisation and Dropout]]"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
