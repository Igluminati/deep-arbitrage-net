{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "630c30d4",
   "metadata": {},
   "source": [
    "[[Neural Networks from Scratch]]\n",
    "\n",
    "##### Initialise the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c802de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import micropip\n",
    "\n",
    "await micropip.install(\"numpy\")\n",
    "await micropip.install(\"nnfs\")\n",
    "await micropip.install(\"matplotlib\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "from nnfs.datasets import vertical_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21b12db",
   "metadata": {},
   "source": [
    "##### Running the \"vertical dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c8af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnfs.init()\n",
    "\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap='brg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368426f9",
   "metadata": {},
   "source": [
    "##### Running the \"spiral dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4953a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap='brg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb0e684",
   "metadata": {},
   "source": [
    "\n",
    "##### Model construction, loss definition, and brute-force search over 100,000 random hidden-layer weight-bias combinations to record minimal loss and corresponding parameters (Requires the classes we created in \"Implementing Loss.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787c8804",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1 = Layer_Dense(2,3)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(3,3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "lower_loss = 9999999\n",
    "best_dense1_weights = dense1.weights.copy()\n",
    "best_dense1_biases = dense1.biases.copy()\n",
    "best_dense2_weights = dense2.weights.copy()\n",
    "best_dense2_biases = dense2.biases.copy()\n",
    "\n",
    "for iteration in range(100000):\n",
    "\tdense1.weights = 0.05 * np.random.randn(2,3)\n",
    "\tdense1.biases = 0.05 * np.random.randn(1,3)\n",
    "\tdense2.weights = 0.05 * np.random.randn(3,3)\n",
    "\tdense2.biases = 0.05 * np.random.randn(1,3)\n",
    "\n",
    "\tdense1.forward(X)\n",
    "\tactivation1.forward(dense1.output)\n",
    "\tdense2.forward(activation1.output)\n",
    "\tactivation2.forward(dense2.output)\n",
    "\n",
    "\tloss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "\tpredictions = np.argmax(activation2.output, axis=1)\n",
    "\taccuracy = np.mean(predictions==y)\n",
    "\n",
    "\tif loss < lower_loss:\n",
    "\t\tprint('New set of weights found, iteration:', iteration,\n",
    "\t\t'loss:', loss, 'acc:', accuracy)\n",
    "\t\tbest_dense1_weights = dense1.weights.copy()\n",
    "\t\tbest_dense1_biases = dense1.biases.copy()\n",
    "\t\tbest_dense2_weights = dense2.weights.copy()\n",
    "\t\tbest_dense2_biases = dense2.biases.copy()\n",
    "\t\tlowest_loss = loss\n",
    "\telse:\n",
    "\t\tdense1.weights = best_dense1_weights.copy()\n",
    "\t\tdense1.biases = best_dense1_biases.copy()\n",
    "\t\tdense2.weights = best_dense2_weights.copy()\n",
    "\t\tdense2.biases = best_dense2_biases.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888e9e9d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**The process above of randomly searching for weights and biases until we reach a local minimum is INEFFICIENT and has a worst case time complexity of O(n) where n is the number of random initialisations. With 1 million samples, if each loss evaluation took 1 millisecond, total time would be 1 million milliseconds which is 1,000 seconds or approximately 16 minutes and 40 seconds.**\n",
    "\n",
    "##### Calculating the derivative (slope of tangent line) at various points in the graph $f(x)=2x^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee857a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the function\n",
    "def f(x):\n",
    "    return 2 * x**2\n",
    "\n",
    "# Generate x values\n",
    "x = np.arange(0, 5, 0.001)\n",
    "y = f(x)\n",
    "\n",
    "# Plot the original function\n",
    "plt.plot(x, y)\n",
    "\n",
    "# Define colours for tangent lines\n",
    "colors = ['k', 'g', 'r', 'b', 'c']\n",
    "\n",
    "# Function to calculate tangent line\n",
    "def approximate_tangent_line(x, approximate_derivative, b):\n",
    "    return (approximate_derivative * x) + b\n",
    "\n",
    "# Draw tangents at integer x-values from 0 to 4\n",
    "for i in range(5):\n",
    "    p2_delta = 0.0001\n",
    "    x1 = i\n",
    "    x2 = x1 + p2_delta\n",
    "    y1 = f(x1)\n",
    "    y2 = f(x2)\n",
    "    \n",
    "    print((x1, y1), (x2, y2))\n",
    "    \n",
    "    approximate_derivative = (y2 - y1) / (x2 - x1)\n",
    "    b = y2 - (approximate_derivative * x2)\n",
    "    \n",
    "    to_plot = [x1 - 0.9, x1, x1 + 0.9]\n",
    "    \n",
    "    plt.scatter(x1, y1, c=colors[i])\n",
    "    plt.plot(\n",
    "        [point for point in to_plot],\n",
    "        [approximate_tangent_line(point, approximate_derivative, b) for point in to_plot],\n",
    "        c=colors[i]\n",
    "    )\n",
    "    \n",
    "    print(f'Approximate derivative for f(x) where x = {x1} is {approximate_derivative}')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4136a781",
   "metadata": {},
   "source": [
    "For the simple function, $f(x) = 2x^2$, we didn't pay a high penalty by approximating the derivative (the slope of the tangent line) like this, and received a value that was close enough for our needs.\n",
    "\n",
    "The *actual* function employed in our neural network is not so simple. The loss function contains all of the layers, weights, and biases - it's a massive function operating in multiple dimensions. Calculating derivatives using numerical differentiation requires multiple forward passes for a single parameter update.\n",
    "\n",
    "To reiterate, as we quickly covered many terms, the derivative is the slope of the tangent line for a function that takes a single parameter as an input. We’ll use this ability to calculate the slopes of the loss function at each of the weight and bias points — this brings us to the multivariate function, which is a function that takes multiple parameters and is a topic for the next chapter — the partial derivative\n",
    "\n",
    "##### The Partial Derivative\n",
    "The partial derivative measures how much impact a single input has on a function's output. The method of calculation is the same as for derivatives explained in the previous chapter; we simply repeat this process for each of the independent inputs.\n",
    "\n",
    "$$\n",
    "f(x, y, z) \\\\\n",
    "\\frac{\\partial f}{\\partial x}, \\quad \\frac{\\partial f}{\\partial y}, \\quad \\frac{\\partial f}{\\partial z} \\\\\n",
    "\\nabla f = \\left[ \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z} \\right]\n",
    "$$\n",
    "In simple terms, a partial derivative tells you how much a function changes if you nudge just one input, while keeping all other inputs frozen.\n",
    "\n",
    "The **partial derivative of the sum** with respect to any input equals 1:\n",
    "$$\n",
    "f(x, y) = x + y\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x} f(x, y) = 1\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial}{\\partial y} f(x, y) = 1\n",
    "$$\n",
    "The **partial derivative of the multiplication** operation with 2 inputs, with respect to any input, equals the other input:\n",
    "$$\n",
    "f(x, y) = x * y\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x} f(x, y) = y\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial}{\\partial y} f(x, y) = 1=x\n",
    "$$\n",
    "The **partial derivative of the max function of 2 variables** with respect to any of them is 1 if this variable is the biggest and 0 otherwise. An example of x:**\n",
    "\n",
    "$$\n",
    "f(x, y) = max(x,y)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x} f(x, y) = 1(x>y)\n",
    "$$\n",
    "The **derivative of the max function of a single variable** and 0 equals 1 if the variable is greater than 0 and 0 otherwise:\n",
    "$$\n",
    "f(x) = max(x,0)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x} f(x) = 1(x>0)\n",
    "$$\n",
    "The **derivative of chained functions** equals the product of the partial derivatives of the subsequent functions:\n",
    "$$\n",
    "\\frac{d}{dx} f(g(x)) = \\frac{d}{dg(x)}f(g(x))\\times \\frac{d}{dx}g(x) = f'(g(x))\\times g'(x)\n",
    "$$\n",
    "The **same applies to the partial derivatives**. For example:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x}f(g(y,h(x,z))) = f'(g(y,h(x,z)))\\times g'(y,h(x,z))\\times h'(x,z)\n",
    "$$\n",
    "The **gradient is a vector of all possible partial derivatives**. An example of a triple-input function:\n",
    "$$\n",
    "\\nabla f =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial}{\\partial x}(x,y,z) \\\\\n",
    "\\frac{\\partial}{\\partial y}(x,y,z) \\\\\n",
    "\\frac{\\partial}{\\partial z}(x,y,z)\n",
    "\\end{bmatrix} \n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial}{\\partial x} \\\\\n",
    "\\frac{\\partial}{\\partial y} \\\\\n",
    "\\frac{\\partial}{\\partial z}\n",
    "\\end{bmatrix}\n",
    "f(x,y,z)\n",
    "$$\n",
    "\n",
    "##### What is the purpose of all the functions summarised above?\n",
    "To gain an idea of how to measure the impact of variables on a function's output, we can begin to write the code to calculate these partial derivatives to see their role in minimising the model's loss."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
