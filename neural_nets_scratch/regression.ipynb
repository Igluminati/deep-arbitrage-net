{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bfc453e",
   "metadata": {},
   "source": [
    "[[Neural Networks from Scratch]]\n",
    "\n",
    "##### Initialise the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9224faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import micropip\n",
    "\n",
    "await micropip.install(\"numpy\")\n",
    "await micropip.install(\"nnfs\")\n",
    "await micropip.install(\"matplotlib\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import sine_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb411f67",
   "metadata": {},
   "source": [
    "\n",
    "##### Introduction\n",
    "We've been working with classification models up to this point, where we try to determine *what something is*. Now we're delving into determining a *specific* value based on an input. For instance, we may want to prediction what the temperature will be tomorrow at 0600 or what the price of a car should be in 2032. \n",
    "\n",
    "For this approach, we'll need a new way to measure loss, and a new output layer activation function. The data will also be different, we need training data that has target scalar values, not classes.\n",
    "\n",
    "##### Producing a Sine Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e496d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnfs.init()\n",
    "\n",
    "X, y = sine_data()\n",
    "\n",
    "plt.plot(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7353cb9f",
   "metadata": {},
   "source": [
    "If we're training on the sine wave pattern above, each input `X` maps to a scalar output `y`.\n",
    "\n",
    "##### Activation Function for Output Layer\n",
    "Regression tasks don't use Softmax or Sigmoid at the output. We use a *Linear Activation*, i.e. output = input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c98990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Linear:\n",
    "\tdef forward(self, inputs):\n",
    "\t\tself.inputs = inputs\n",
    "\t\tself.output = output\n",
    "\n",
    "\tdef backward(self, dvalues):\n",
    "\t\tself.dinputs = dvalues.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc58030c",
   "metadata": {},
   "source": [
    "No transformation during the forward pass. The gradient of `y=x` is 1, so during backprop, gradient passes unchanged.\n",
    "\n",
    "##### Loss Functions for Regression\n",
    "###### Loss Class which is the parent object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e83741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "\tdef calculate(self, output, y):\n",
    "\t\tsample_losses = self.forward(output, y)\n",
    "\t\tdata_loss = np.mean(sample_losses)\n",
    "\t\treturn data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89925e9",
   "metadata": {},
   "source": [
    "\n",
    "We use two loss functions for Regression:\n",
    "###### Mean Squared Error (MSE)\n",
    "$$\n",
    "MSE = \\frac{1}{n}\\sum^n_{i=1}(y_i-\\tilde{y}_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b02b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_MeanSquaredError(Loss):\n",
    "\tdef forward(self, y_pred, y_true):\n",
    "\t\tsample_losses = np.mean((y_true - y_pred) ** 2, axis=-1)\n",
    "\t\treturn sample_losses\n",
    "\n",
    "\tdef backward(self, dvalues, y_true):\n",
    "\t\tsamples = len(dvalues)\n",
    "\t\toutputs = len(dvalues[0])\n",
    "\t\tself.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "\t\tself.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459c0196",
   "metadata": {},
   "source": [
    "Harsh on large errors due to the square.\n",
    "\n",
    "###### Mean Absolute Error (MAE)\n",
    "$$\n",
    "MAE = \\frac{1}{n}\\sum^n_{i=1}|y_i-\\tilde{y}_i|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8c9309",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_MeanAbsoluteError(Loss):\n",
    "\tdef forward(self, y_pred, y_true):\n",
    "\t\tsample_losses = np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "\t\treturn sample_losses\n",
    "\n",
    "\tdef backward(self, dvalues, y_true):\n",
    "\t\tsamples = len(dvalues)\n",
    "\t\toutputs = len(dvalues[0])\n",
    "\t\tself.dinputs = np.sign(y_true - dvalues) / outputs\n",
    "\t\tself.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4d628d",
   "metadata": {},
   "source": [
    "Less sensitive to outliers, but less smooth gradient.\n",
    "\n",
    "##### Accuracy for Regression#\n",
    "A prediction in Regression is \"correct\" if it's within a precision range (standard deviation) of the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4abefb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_precision = np.std(y) / 250\n",
    "accuracy = np.mean(np.absolute(predictions - y) < accuracy_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8be8512",
   "metadata": {},
   "source": [
    "\n",
    "##### Typical Model Architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe2570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1 = Layer_Dense(1, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64, 64)\n",
    "activation2 = Activation_ReLU()\n",
    "dense3 = Layer_Dense(64, 1)\n",
    "activation3 = Activation_Linear()\n",
    "\n",
    "loss_function = Loss_MeanSquaredError()\n",
    "optimiser = Optimiser_Adam()\n",
    "\n",
    "accuracy_precision = np.std(y) / 250"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef90202",
   "metadata": {},
   "source": [
    "\n",
    "##### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da61f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10001):\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "    dense3.forward(activation2.output)\n",
    "    activation3.forward(dense3.output)\n",
    "\n",
    "    data_loss = loss_function.calculate(activation3.output, y)\n",
    "    regularisation_loss = (\n",
    "        loss_function.regularisation_loss(dense1)\n",
    "        + loss_function.regularisation_loss(dense2)\n",
    "        + loss_function.regularisation_loss(dense3)\n",
    "    )\n",
    "    loss = data_loss + regularisation_loss\n",
    "\n",
    "    predictions = activation3.output\n",
    "    accuracy = np.mean(np.absolute(predictions - y) < accuracy_precision)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f} (data_loss: {data_loss:.3f}, reg_loss: {regularisation_loss:.3f}), lr: {optimiser.current_learning_rate}')\n",
    "\n",
    "    loss_function.backward(activation3.output, y)\n",
    "    activation3.backward(loss_function.dinputs)\n",
    "    dense3.backward(activation3.dinputs)\n",
    "    activation2.backward(dense3.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    optimiser.pre_update_params()\n",
    "    optimiser.update_params(dense1)\n",
    "    optimiser.update_params(dense2)\n",
    "    optimiser.update_params(dense3)\n",
    "    optimiser.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a7ca0",
   "metadata": {},
   "source": [
    "\n",
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2ce8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = sine_data()\n",
    "\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "dense3.forward(activation2.output)\n",
    "activation3.forward(dense3.output)\n",
    "\n",
    "plt.plot(X_test, y_test)\n",
    "plt.plot(X_test, activation3.output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3398754b",
   "metadata": {},
   "source": [
    "Compares true values with predictions visually.![[Screenshot_2025-06-06_11-28-23.png]]\n",
    "##### Next Step\n",
    "[[Model Object]]"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
